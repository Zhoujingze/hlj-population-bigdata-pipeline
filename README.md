# 城市人口数据采集系统以黑龙江省为例
大数据处理项目，包含数据生成、Hive存储优化、统计分析等功能

# 大数据处理项目说明文档

## 项目概述

本项目是一个完整的大数据处理流水线，主要功能包括：
1. **数据生成**：使用Python生成模拟的手机用户数据
2. **数据导入**：通过Shell脚本将数据导入Hive数据库
3. **存储优化**：创建多种存储格式的Hive表并比较性能
4. **统计分析**：对Hive中的数据进行统计分析
5. **数据同步**：将统计结果同步到MySQL数据库

## 系统架构分层

### 1. **数据源层**
- **Excel文件**：行政区划数据（黑龙江.xlsx）
- **配置数据**：城市信息配置（CITY_INFO数组）
- **外部数据**：全国行政区划数据（全国.xlsx）

### 2. **数据存储层**
- **文件存储**：
  - 本地文件系统：生成的城市数据文件（data目录下的各城市.txt文件）
  - HDFS：原始数据（/data/cityFile/city.txt）、Hive表数据、统计结果
- **数据库存储**：
  - Hive数据仓库：三种存储格式的表（city_textfile、city_rcfile、city_orc）
  - MySQL关系型数据库：最终统计结果（city_statistics表）

### 3. **数据处理层**
- **数据生成**：Python脚本（数据模拟和格式化）
- **数据合并**：Python脚本（将各城市数据文件合并为统一文件）
- **数据导入**：Shell脚本（Hive表创建和数据加载）
- **数据转换**：Shell脚本（存储格式转换：TextFile→RCFile→ORC）
- **数据分析**：Shell脚本（HiveQL统计分析）
- **数据同步**：Shell脚本（Hive→HDFS→本地→MySQL）
- **空间监控**：Python脚本（计算各数据文件存储空间）

### 4. **应用访问层**
- **命令行接口**：
  - Hive CLI/Beeline：Hive数据查询
  - MySQL命令行：统计结果查询
  - HDFS命令行：文件系统操作
- **Web管理界面**：
  - HDFS NameNode Web UI：HDFS文件管理
  - Hive Web UI（如Hue）：Hive表管理
  - MySQL管理工具（如phpMyAdmin）：数据库管理

### 5. **监控管理层**
- **存储空间监控**：HDFS表大小统计
- **执行日志监控**：脚本执行过程日志
- **数据质量检查**：数据完整性和一致性验证

## 技术架构

### 1. 数据生成模块 (`generate_city_data.py`)

**功能描述**：
- 读取黑龙江省行政区划数据
- 为13个地级市生成模拟手机用户数据
- 每个城市生成独立的文本文件

**Excel数据读取**：
- 使用pandas读取Excel文件，确保行政区划代码以字符串形式保留前导零
- 过滤有效区划代码（6位且不以"00"结尾）
- 根据前4位代码将区划代码分配到对应城市
- 处理反向映射查找城市名称

**数据生成逻辑**：
- 检查手机号容量（确保人口不超过1亿）
- 使用numpy高效生成唯一手机号后缀
- 随机生成年龄（16-100岁）、性别（0-1，代表男女）和行政区划代码

**文件输出**：
- 为每个城市创建独立文件
- 确保数据格式为：`手机号|年龄|性别|区划代码|0`

**使用前准备**：
```bash
# 安装依赖
pip install pandas openpyxl numpy
```

**环境要求**：
- Excel文件与脚本在同一目录
- 文件名为"黑龙江.xlsx"
- Excel包含"行政区划代码"和"行政区名称"两列

**城市配置信息**：
```python
CITY_INFO = [
    {'城市': '哈尔滨市', '人口数': 10009900, '代码': '2301', '手机号头': '138'},
    {'城市': '绥化市', '人口数': 3756200, '代码': '2312', '手机号头': '130'},
    {'城市': '齐齐哈尔市', '人口数': 4067500, '代码': '2302', '手机号头': '136'},
    {'城市': '大庆市', '人口数': 2781600, '代码': '2306', '手机号头': '150'},
    {'城市': '牡丹江市', '人口数': 2290200, '代码': '2310', '手机号头': '166'},
    {'城市': '佳木斯市', '人口数': 2156500, '代码': '2308', '手机号头': '154'},
    {'城市': '鸡西市', '人口数': 1502100, '代码': '2303', '手机号头': '137'},
    {'城市': '黑河市', '人口数': 1286400, '代码': '2311', '手机号头': '132'},
    {'城市': '双鸭山市', '人口数': 1208800, '代码': '2305', '手机号头': '189'},
    {'城市': '鹤岗市', '人口数': 891300, '代码': '2304', '手机号头': '139'},
    {'城市': '伊春市', '人口数': 878900, '代码': '2307', '手机号头': '151'},
    {'城市': '七台河市', '人口数': 689600, '代码': '2309', '手机号头': '155'},
    {'城市': '大兴安岭地区', '人口数': 331300, '代码': '2327', '手机号头': '131'},
]
```

**生成的数据格式示例**：
```
13806827534|85|1|230126|0
```

### 2. Hive数据导入模块 (`import_to_hive.sh`)

**功能描述**：
- 创建Hive数据库和表结构
- 将生成的数据导入Hive
- 创建多种存储格式表进行性能对比
- 统计各存储格式的空间占用

**表结构定义**：
- 所有表包含相同字段：`phone, age, gender, adcode, dummy`
- 指定字段分隔符为 `|`（仅TextFile表需要）
- 分别使用三种存储格式：TEXTFILE, RCFILE, ORC

**数据加载流程**：
- 先将数据加载到TextFile表（直接加载原始文件）
- 通过 `INSERT OVERWRITE` 将数据转换到RCFile和ORC表

**空间大小计算**：
- 使用 `hdfs dfs -du` 命令查看HDFS目录大小
- 表路径默认为 `/user/hive/warehouse/<表名>`

**脚本使用说明**：
```bash
# 保存脚本为import_to_hive.sh
chmod +x import_to_hive.sh
./import_to_hive.sh
```

**预期输出示例**：
```
===== 表存储空间大小 =====

表名: city_textfile
12.3 K  /user/hive/warehouse/city_textfile/city.txt
------------------------
表名: city_rcfile
8.1 K   /user/hive/warehouse/city_rcfile/000000_0
------------------------
表名: city_orc
4.7 K   /user/hive/warehouse/city_orc/000000_0
```

**存储格式差异**：
- **TextFile**：明文存储，占用空间最大，可直接查看内容
- **RCFile**：列式存储，压缩率高，适合查询部分列
- **ORC**：优化行列存储，压缩率最高，适合复杂查询

### 2. 数据合并模块 (`合并.py`)

**功能描述**：
- 将各城市生成的独立数据文件合并为一个统一的数据文件
- 为Hive导入准备数据源
- 统计合并后的数据总量

**核心特性**：
- **自动化合并**：自动扫描并合并data目录下所有.txt文件
- **数据完整性**：保持原始数据格式不变
- **统计功能**：自动统计合并后的总数据条数

**技术实现**：
```python
# 主要功能函数
def merge_files(input_dir, output_file):
    # 合并指定目录下的所有.txt文件
    
def count_lines(file_path):
    # 统计文件中的总行数
```

**数据处理流程**：
1. 扫描data目录下所有.txt文件
2. 按文件顺序读取并合并内容
3. 生成统一的city.txt文件
4. 统计并显示总数据条数

**生成的数据格式**：
```
13806827534|85|1|230126|0
13607548291|32|0|230203|0
...
```

### 3. 存储空间监控模块 (`计算内存空间.py`)

**功能描述**：
- 计算并显示各数据文件的存储空间占用
- 帮助了解数据规模和存储需求
- 为存储优化提供参考依据

**核心特性**：
- **自动扫描**：自动扫描指定目录下的所有文件
- **多单位显示**：同时显示字节和MB单位
- **格式化输出**：清晰易读的输出格式

**技术实现**：
```python
# 主要操作步骤
1. 检查文件夹是否存在
2. 遍历文件夹中的所有文件
3. 获取每个文件的大小（字节）
4. 转换为MB并格式化输出
```

**输出示例**：
```
文件：哈尔滨市.txt，大小：123456789 字节 (117.74 MB)
文件：齐齐哈尔市.txt，大小：98765432 字节 (94.18 MB)
...
```

### 4. 数据统计分析模块 (`sync_to_mysql.sh`)

**功能描述**：
- 在Hive中执行复杂统计分析
- 按行政区划统计性别分布和年龄分布
- 将统计结果同步到MySQL数据库

**Hive统计逻辑**：
- 按行政区划代码 (adcode) 分组统计
- 性别统计：
  - `male_count`: gender=0的数量（男性）
  - `female_count`: gender=1的数量（女性）
- 年龄分段统计：
  - 0-18岁
  - 19-45岁
  - 46-60岁
  - 60岁以上

**脚本使用说明**：
```bash
# 保存脚本为sync_to_mysql.sh
chmod +x sync_to_mysql.sh

# 修改MySQL配置
# 确保以下参数与实际环境一致：
MYSQL_HOST="localhost"        # MySQL服务器地址
MYSQL_PORT="3306"            # MySQL端口
MYSQL_USER="root"            # MySQL用户名
MYSQL_PASS="123456"          # MySQL密码

# 运行脚本
./sync_to_mysql.sh
```

**验证步骤**：

**1. 检查Hive统计结果**
```bash
# 查看Hive统计输出
hdfs dfs -cat /data/city_stat_result/*
```

**输出示例**：
```
230223,15,12,5,20,8,3
230205,20,18,7,25,10,4
```

**2. 查询MySQL数据**
```bash
mysql -u root -p123456 -e "SELECT * FROM city_stats.city_statistics;"
```

**预期输出**：
```
+---------+------------+--------------+-----------+-------------+-------------+--------------+---------------------+
| adcode  | male_count | female_count | age_0_18  | age_19_45   | age_46_60   | age_60_plus  | update_time         |
+---------+------------+--------------+-----------+-------------+-------------+--------------+---------------------+
| 230223  | 15         | 12           | 5         | 20          | 8           | 3            | 2023-10-01 14:30:00 |
| 230205  | 20         | 18           | 7         | 25          | 10          | 4            | 2023-10-01 14:30:00 |
+---------+------------+--------------+-----------+-------------+-------------+--------------+---------------------+
```

**MySQL表结构**：
```sql
CREATE TABLE city_statistics (
    adcode VARCHAR(20) PRIMARY KEY COMMENT '行政区划代码',
    male_count INT COMMENT '男性数量',
    female_count INT COMMENT '女性数量',
    age_0_18 INT COMMENT '0-18岁数量',
    age_19_45 INT COMMENT '19-45岁数量',
    age_46_60 INT COMMENT '46-60岁数量',
    age_60_plus INT COMMENT '60岁以上数量',
    update_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '更新时间'
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

**执行步骤**：
1. 在Hive中执行统计查询
2. 将HDFS结果导出到本地
3. 创建MySQL数据库和表
4. 导入数据到MySQL
5. 清理临时文件

## 数据流程图

```
Excel数据 → 生成脚本 → 各城市文本文件
                    ↓
                合并脚本 → city.txt
                    ↓
                HDFS上传 → /data/cityFile/city.txt
                    ↓
                Hive导入脚本
                    ↓
                Hive数据库
                /    |    \
          TextFile RCFile ORC
                    ↓
                统计分析脚本
                    ↓
                MySQL数据库
```

## 配置参数说明

### Python脚本配置
- `DATA_DIR`: 数据输出目录
- `EXCEL_PATH`: Excel数据文件路径
- `CITY_INFO`: 城市信息配置数组（包含城市名、人口数、代码、手机号头）

### Shell脚本2配置
- `DB_NAME`: Hive数据库名称
- `HDFS_DATA_PATH`: HDFS中原始数据路径

### Shell脚本3配置
- `HIVE_DB`: Hive数据库名称
- `HDFS_RESULT_PATH`: HDFS统计结果路径
- `LOCAL_TMP_FILE`: 本地临时文件路径
- `MYSQL_HOST`: MySQL主机地址
- `MYSQL_PORT`: MySQL端口
- `MYSQL_USER`: MySQL用户名
- `MYSQL_PASS`: MySQL密码
- `MYSQL_DB`: MySQL数据库名
- `MYSQL_TABLE`: MySQL表名

## 使用说明

### 1. 环境准备
- Python 3.x 环境
- 安装依赖：`pip install pandas numpy openpyxl`
- Hadoop和Hive环境
- MySQL数据库

### 2. 执行步骤

#### Windows环境（PyCharm）操作：
```bash
# 步骤1：生成数据
python generate_city_data.py

# 步骤2：合并数据文件
python 合并.py

# 步骤3：查看文件存储空间（可选）
python 计算内存空间.py
```
**执行说明：**
- **步骤1**：在PyCharm中运行，生成各城市的独立数据文件
- **步骤2**：将所有城市数据合并为一个统一的city.txt文件
- **步骤3**：查看各数据文件的存储空间占用（可选步骤）

#### 数据传输到Linux环境：
```bash
# 使用MobaXterm_Personal_23.2将生成的city.txt文件传输到node01虚拟机
# 传输路径：/home/用户名/ 或指定目录
```

#### Linux环境（node01虚拟机）操作：
```bash
# 步骤4：登录HDFS并上传数据
hdfs dfs -mkdir -p /data/cityFile
hdfs dfs -put /path/to/city.txt /data/cityFile/

# 步骤5：导入Hive并创建多格式表
bash import_to_hive.sh

# 步骤6：执行统计并同步到MySQL
bash sync_to_mysql.sh
```
**执行说明：**
- **步骤4**：将city.txt文件上传到HDFS，为Hive导入做准备
- **步骤5**：在Hive中创建数据库和表，并导入数据
- **步骤6**：执行统计分析并将结果同步到MySQL

#### 完整流程总结：
1. **Windows环境**：使用PyCharm生成和合并数据文件
2. **数据传输**：通过MobaXterm将city.txt传输到Linux虚拟机
3. **Linux环境**：在node01虚拟机上执行Hadoop和MySQL相关操作
4. **HDFS操作**：上传数据到HDFS分布式文件系统
5. **Hive处理**：创建多格式表并进行数据导入
6. **统计分析**：执行HiveQL查询并同步结果到MySQL

### 3. 数据验证
- 检查Hive中各表的记录数
- 验证MySQL中的统计结果
- 对比不同存储格式的空间占用

## 数据示例

### 输入数据（黑龙江行政区划）
```
序号	行政区名称	行政区划代码	数值
13	黑龙江省	230,000	100,000

行政区划代码	行政区名称
230,102	道里
230,103	南岗
230,104	道外
230,108	平房
```

### 生成的数据格式
```
13806827534|85|1|230126|0
```

### 统计结果示例
```
adcode,male_count,female_count,age_0_18,age_19_45,age_46_60,age_60_plus
230102,1250,1180,450,1200,600,180
230103,1100,1050,380,1050,580,140
...
```

## 技术亮点

1. **数据生成优化**：使用NumPy的随机数生成确保性能和唯一性
2. **存储格式对比**：同时支持TextFile、RCFile、ORC三种格式
3. **自动化流程**：Shell脚本实现全自动化数据处理
4. **错误处理**：包含文件存在性检查和异常处理
5. **资源管理**：自动清理临时文件，避免资源浪费
6. **数据合并自动化**：智能合并多个城市数据文件，保持数据完整性
7. **存储空间监控**：实时监控各数据文件存储占用，为优化提供依据
8. **完整数据流水线**：从数据生成到最终统计的完整处理链路

## 扩展建议

1. **性能优化**：可以考虑使用Spark进行大规模数据处理
2. **监控增强**：添加执行时间统计和性能监控
3. **数据质量**：增加数据验证和清洗步骤
4. **调度自动化**：集成到调度系统（如Airflow）中
5. **可视化**：添加数据可视化展示功能

## 注意事项

1. **数据文件路径**：确保Excel文件路径正确
2. **HDFS权限**：确保有足够的HDFS操作权限
3. **MySQL连接**：确保MySQL连接参数正确
4. **内存管理**：大规模数据处理时注意内存使用
5. **网络环境**：确保Hadoop集群网络连接正常

## 故障排除

### 常见问题
1. **Excel文件读取失败**：检查文件路径和格式
2. **Hive连接失败**：检查Hive服务状态
3. **MySQL连接失败**：检查MySQL服务状态和连接参数
4. **HDFS权限问题**：检查HDFS用户权限
5. **内存不足**：调整JVM内存配置

### 调试建议
1. 逐步执行每个脚本，验证中间结果
2. 查看日志输出，定位错误信息
3. 使用Hive和MySQL命令行工具手动验证
4. 检查系统资源使用情况

## 总结

这个项目展示了完整的大数据处理流程，从数据生成到最终的数据存储和统计分析，是一个很好的大数据技术实践案例。项目涵盖了数据生成、数据导入、存储优化、统计分析、数据同步等关键环节，适合作为大数据课程设计的参考案例。

---
